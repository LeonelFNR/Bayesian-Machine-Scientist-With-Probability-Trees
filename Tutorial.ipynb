{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for the Bayesian machine scientist "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial illustrates how to program a Bayesian machine scientist, using the code provided here. The tutorial assumes general knowledge of Python programming. We start by importing all necessary Python modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "sys.path.append('./')\n",
    "sys.path.append('./Prior/')\n",
    "from mcmc import *\n",
    "from parallel import *\n",
    "from fit_prior import read_prior_par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the data. In this particular case, we load the salmon stocks data. The features (independent variables) are loaded into a Pandas `DataFrame` named `x`, whereas the target (dependent) variable is loaded into a Pandas `Series` named `y`. Data should **always** be loaded in these formats to avoid problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XLABS = [\n",
    "    'eff',\n",
    "    'D_max',\n",
    "    'D_apr',\n",
    "    'D_may',\n",
    "    'D_jun',\n",
    "    'ET_apr',\n",
    "    'ET_may',\n",
    "    'ET_jun',\n",
    "    'PT_apr',\n",
    "    'PT_may',\n",
    "    'PT_jun',\n",
    "    'PT_jul',\n",
    "    'PDO_win',\n",
    "]\n",
    "raw_data = pd.read_csv('Validation/LogYe/data/seymour.csv')\n",
    "x, y = raw_data[XLABS], np.log(raw_data['rec'])\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Bayesian machine scienstist "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by initializing the machine scientist. This involves three steps:\n",
    "- **Reading the prior hyperparameters.** The values of the hyperparameters depend on the number of variables `nv` and parameters `np`considered during the search. Many combinations of `nv` and `np` have hyperparameters calculated in the `Prior` directory. Otherwise, the hyperparameters should be fit. \n",
    "- **Setting the \"temperatures\" for the parallel tempering.** If you don't know what parallel tempering is, you can read it in the Methods section of the paper, or just leave it as is in the code. In general, more temperatures (here 20) lead to better sampling of the expression space (we use a maximum of 100 different temperatures)\n",
    "- **Initializing the (parallel) scientist.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the hyperparameters for the prior\n",
    "prior_par = read_prior_par('./Prior/final_prior_param_sq.named_equations.nv13.np13.2016-09-01 17:05:57.196882.dat')\n",
    "\n",
    "# Set the temperatures for the parallel tempering\n",
    "Ts = [1] + [1.04**k for k in range(1, 20)]\n",
    "\n",
    "# Initialize the parallel machine scientist\n",
    "pms = Parallel(\n",
    "    Ts,\n",
    "    variables=XLABS,\n",
    "    parameters=['a%d' % i for i in range(13)],\n",
    "    x=x, y=y,\n",
    "    prior_par=prior_par,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling expressions with the Bayesian machine scientist "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start sampling expressions with the Bayesian machine scientist, using MCMC. In its simplest form, one just needs to run the `mcmc_step()` and the `tree_swap()` methods as many times as necessary. `mcmc_step()` performs an MCMC update at each of the temperatures of the parallel tempering, whereas `tree_swap()` attempts to swap the expressions at two consecutive temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of MCMC steps\n",
    "nstep = 100\n",
    "\n",
    "# Draw a progress bar to keep track of the MCMC progress\n",
    "f = IntProgress(min=0, max=nstep, description='Running:') # instantiate the bar\n",
    "display(f)\n",
    "\n",
    "# MCMC\n",
    "for i in range(nstep):\n",
    "    # MCMC update\n",
    "    pms.mcmc_step() # MCMC step within each T\n",
    "    pms.tree_swap() # Attempt to swap two randomly selected consecutive temps\n",
    "    # Update the progress bar\n",
    "    f.value += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, of course, one wants to do something other than just generate expressions. For example, one may want to keep track of the most plausible (or, equivalently, the minimum description length) model visited so far by the MCMC, or to keep a trace of some of the properties of the sampled expressions. The example below keeps the best model, as well as a trace of all the description lengths visited. Note that, in `Parallel` objects, the relevant expression is stored in the `t1` attribute (which stands for temperature 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of MCMC steps\n",
    "nstep = 3000\n",
    "\n",
    "# Draw a progress bar to keep track of the MCMC progress\n",
    "f = IntProgress(min=0, max=nstep, description='Running:') # instantiate the bar\n",
    "display(f)\n",
    "\n",
    "# MCMC\n",
    "description_lengths, mdl, mdl_model = [], np.inf, None\n",
    "for i in range(nstep):\n",
    "    # MCMC update\n",
    "    pms.mcmc_step() # MCMC step within each T\n",
    "    pms.tree_swap() # Attempt to swap two randomly selected consecutive temps\n",
    "    # Add the description length to the trace\n",
    "    description_lengths.append(pms.t1.E)\n",
    "    # Check if this is the MDL expression so far\n",
    "    if pms.t1.E < mdl:\n",
    "        mdl, mdl_model = pms.t1.E, deepcopy(pms.t1)\n",
    "    # Update the progress bar\n",
    "    f.value += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's take a look at the objects we stored. Here is the best model sampled by the machine scientist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best model:\\t', mdl_model)\n",
    "print('Desc. length:\\t', mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the trace of the description length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(description_lengths)\n",
    "plt.xlabel('MCMC step', fontsize=14)\n",
    "plt.ylabel('Description length', fontsize=14)\n",
    "plt.title('MDL model: $%s$' % mdl_model.latex())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with the Bayesian machine scientist "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we typically want to make predictions with models. In this regard, the interface of the machine scientist is similar to those in Scikit Learn: to make a prediction we call the `predict(x)` method, with an argument that has the same format as the training `x`, that is, a Pandas `DataFrame` with the exact same columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(mdl_model.predict(x), y)\n",
    "plt.plot((-6, 0), (-6, 0))\n",
    "plt.xlabel('MDL model predictions', fontsize=14)\n",
    "plt.ylabel('Actual values', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further capabilities of the Bayesian machine scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading an expression into the machine scientist\n",
    "\n",
    "Rather than sampling models, we can directly build a machine scientist from a given expression. This is useful when we want to evaluate models that have been previously saved, or to analyze specific predefined models. For this we do not use `Parallel` machine scientists but rather a single `Tree` machine scientist; we instantiate the `Tree` with the `from_string` keyword. Additionally, in these situations we typically want to set the values of the model parameters, because trying to fit them from scratch may fail. All in all, we proceed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the string (NOTE this MUST be in the same format as the code write models!)\n",
    "model_string = \"(log((eff / (D_apr * _a0_)))\"\n",
    "# Define parameter values\n",
    "model_parameters = {\n",
    "   '_a0_': 6.655292653177647e-05,\n",
    "}\n",
    "\n",
    "# Instantiate a Tree from the desired string\n",
    "my_model = Tree(\n",
    "    variables=XLABS,\n",
    "    parameters=['a%d' % i for i in range(13)],\n",
    "    x=x, y=y,\n",
    "    prior_par=prior_par,\n",
    "    from_string=model_string,\n",
    ")\n",
    "\n",
    "# Set the parameter values\n",
    "my_model.set_par_values(model_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this model works as any other model. For example, you can use it to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(my_model.predict(x), y)\n",
    "plt.plot((-6, 0), (-6, 0))\n",
    "plt.xlabel('My predefined model predictions', fontsize=14)\n",
    "plt.ylabel('Actual values', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding models for multiple data sets at the same time\n",
    "\n",
    "The machine scientist can model multiple data sets at the same time, using the same models for all data sets, but allowing for different parameter values for each data set. This approach makes sense when one expects the mechanisms to be common accross data sets, but idiosyncrasies of each data set preclude from putting all data together. This approach has been used, for example, to [model human mobility across States within the U.S.](https://arxiv.org/abs/2312.11281), or to [model friction in turbulent flows across different experimental conditions](https://link.aps.org/doi/10.1103/PhysRevLett.124.084503).\n",
    "\n",
    "To achieve this, and assuming that `x1` and `x2` are feature dataframes like `x` above, and that `y1` and `y2` are target dataframes like `y` above, we just need to build the following dictionaries:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmulti = {'dataset1' : x1, 'dataset2' : x2}\n",
    "ymulti = {'dataset1' : y1, 'dataset2' : y2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and feed `x=xmulti` and `y=ymulti` as keyword arguments when instantiating the `Parallel` object, that is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parallel machine scientist with multiple data sets\n",
    "pms = Parallel(\n",
    "    Ts,\n",
    "    variables=XLABS,\n",
    "    parameters=['a%d' % i for i in range(13)],\n",
    "    x=xmulti, y=ymulti,\n",
    "    prior_par=prior_par,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If initialized in this way, `pms.t1.par_values` will automatically be a dictionary with the same keys as `xmulti` and `ymulti` (which, for obvious reasons, have to be the same!), and whose values will be the parameter values for each of the data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing a term to the model\n",
    "\n",
    "Sometimes, for whatever reason, part of the expression we are looking for is known. For example, we may know that the model we are looking for contains a given factor multiplied by something else that we do not know and wish to model. In such cases, it is possible to specify a fixed term to the model. For example, in the example we have been using in this tutorial, we may want to specify that\n",
    "\n",
    "$$\\log({\\rm rec}) = c_1 * \\log ({\\rm eff}) * f({\\rm features})$$\n",
    "\n",
    "and limit our search to $f$. This is achieved by using the `fixed_term` and `fixed_term_op` keyword arguments at the time of the creation of the (single or parallel) machine scientist. \n",
    "\n",
    "There are several ways one may want to use the `fixed_term`, depending on whether the machine scientist needs to use or not in $f$ the features and parameters in the fixed term. If the fixed factor uses some features and parameters that can be used also in $f$, then it can be simply initialized as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parallel machine scientist with an extra factor _a0_*log(eff) that uses\n",
    "# parameters (_a0_) and features (eff)  that are also potentially used in the rest\n",
    "# of the expression\n",
    "pms = Parallel(\n",
    "    Ts,\n",
    "    variables=[xl for xl in XLABS],\n",
    "    parameters=['a%d' % i for i in range(13)],\n",
    "    x=x, y=y,\n",
    "    prior_par=prior_par,\n",
    "    fixed_term='(_a0_ * (log(eff)))', # Note that _a0_ is in parameters and eff is in variables\n",
    "    fixed_term_op='*', # The fixed term will be multiplied to f\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More often, however, we want the parameters and/or features in the fixed term to be excluded from $f$. If we want to use a parameter that cannot be used in $f$, it is enough to use a name for the parameter that is **not** included in `parameters`. (Note, however, that parameter names **must** start and end with `_`.) For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parallel machine scientist with an extra factor _b0_*log(eff) that uses\n",
    "# a *new* parameter (_b0_), and a feature (eff) that is potentially used in the rest\n",
    "# of the expression\n",
    "pms = Parallel(\n",
    "    Ts,\n",
    "    variables=[xl for xl in XLABS],\n",
    "    parameters=['a%d' % i for i in range(13)],\n",
    "    x=x, y=y,\n",
    "    prior_par=prior_par,\n",
    "    fixed_term='(_b0_ * (log(eff)))', # Note that _b0_ is NOT in parameters. An extra parameter,\n",
    "                                      # not usable outside the factor, is created automatically\n",
    "    fixed_term_op='*',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if we want the features in the prefactor **not** to be usable in the rest of the expression, we need to exclude them from `variables` and explicitly add them as a list using the `extra_variables` keyword argument. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parallel machine scientist with an extra factor _b0_*log(eff) that uses\n",
    "# a *new* parameter (_b0_) and a *new* feature (eff) which will *not* appear in the rest\n",
    "# of the expression\n",
    "pms = Parallel(\n",
    "    Ts,\n",
    "    variables=[xl for xl in XLABS if xl != 'eff'], # exclude eff from the list of features\n",
    "    parameters=['a%d' % i for i in range(13)],\n",
    "    x=x, y=y,\n",
    "    prior_par=prior_par,\n",
    "    fixed_term='(_b0_ * (log(eff)))',\n",
    "    fixed_term_op='*',\n",
    "    extra_variables=['eff'], # add eff as an extra feature, only used in the fixed factor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last example, note that both `_b0_` and `eff` will only be used in the fixed term, but not in the rest of the expression. This is, perhaps, the most common situation. \n",
    "\n",
    "**Importantly**, note that the description lengths of models obtained with a fixed term are not directly comparable to models obtained without restriction, as the operations in the fixed factor are not taken into account in the calculation of the description length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further refinements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples above are only intended to illustrate how a basic MCMC would be implemented. In practice, there are other considerations that we kept in mind in all the experiments reported in the manuscriot, and that anyone using the code should too:\n",
    "- **Equilibration**: One should not start sampling until the MCMC has converged to the stationary distribution. Although determining when a sample is in equilibrium, a necessary condition is that the description length is not increasing or, more typically, decreasing. The trace of the description length should be flat (except for fluctuations) before we start collecting samples.\n",
    "- **Thinning**: MCMC samples should be thinned, so only one in, say, 100 samples are kept for the trace. Otherwise, one is getting highly correlated samples, which may lead to, for example, erroneous estimates of confidence intervals.\n",
    "- **Getting trapped**: Despite the parallel tempering, the MCMC can get trapped in local minima of the description length. For this, we typically keep track of the number of steps since the last `tree_swap()` move was accepted for each temperature. If a particular temperature has *not* accepted swaps in a long time, then we anneal the whole system, that is, we increase all temperatures and decrease them slowly back to equilibrium so as to escape the local minima. Using several restarts of the MCMC and comparing the results is also a convenient check.\n",
    "- **Memory issues**: By default, the machine scientist keeps a cache of all visited models, so as to avoid duplicates of previously considered models, as well as to speed up the process of obtaining the maximum likelihood estimators of the model parameters. For long MCMC chains this becomes memory intensive, so it may be convenient to periodically clean this cache (or, at least, old models in this cache) by reinitializing the `fit_pat` and `representative` attributes of the `Parallel` instance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
